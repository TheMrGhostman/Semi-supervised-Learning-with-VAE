{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import time\n",
    "import deepdish.io as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "from utils.inference import SVI\n",
    "import utils.models as m\n",
    "import utils.datasets as d\n",
    "from utils.generative_models import SemiSupervisedGenerativeModel, Generative_Model_Trainer\n",
    "from utils.layers import *\n",
    "from utils.inception import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = np.vstack((np.load(\"data/sequenced_data_for_VAE_length-160_stride-10_pt1.npy\"),\n",
    "               np.load(\"data/sequenced_data_for_VAE_length-160_stride-10_pt2.npy\")))\n",
    "y_s = np.load(\"data/sequenced_data_for_VAE_length-160_stride-10_targets.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u = np.vstack((np.load(\"data/unlabeled_sequences_part_1.npy\"), np.load(\"data/unlabeled_sequences_part_2.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83680, 160)\n",
      "(97400, 160)\n"
     ]
    }
   ],
   "source": [
    "print(X_s.shape)\n",
    "print(X_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X_s), np.array(y_s), test_size=0.2, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaler.fit(np.vstack((np.array(X_s),np.array(X_u))))\n",
    "\n",
    "X_sup= scaler.transform(X_train)\n",
    "X_unsup = scaler.transform(np.array(X_u))\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup = d.H_alphaSequences(X_sup, y_s)\n",
    "test = d.H_alphaSequences(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DIM = 160\n",
    "LATENT_DIM =16\n",
    "Y_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_with_y(nn.Module):\n",
    "    def __init__(self, ORIGINAL_DIM, LATENT_DIM, Y_DIM):\n",
    "        super(Encoder_with_y,self).__init__()\n",
    "        self.conv_enc = nn.Sequential(\n",
    "            Reshape(out_shape=(1, ORIGINAL_DIM)),\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=6, stride=2, padding=0, bias=False), # n*1*160 -> n*16*78\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=4, stride=2, bias=False),#n*16*78->n*32*38\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, stride=2, bias=False),#n*32*38->n*64*18\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            Flatten(out_features=64*18))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*18 +Y_DIM, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            VariationalLayer(in_features=256, out_features=LATENT_DIM, return_KL=False))\n",
    "        \n",
    "    def forward(self, X,y):\n",
    "        cn = self.conv_enc(X)\n",
    "        X = torch.cat([cn,y], axis=1)\n",
    "        #print(X.shape)\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "    Reshape(out_shape=(1, ORIGINAL_DIM)),\n",
    "    nn.Conv1d(in_channels=1, out_channels=16, kernel_size=6, stride=2, padding=0, bias=False), # n*1*160 -> n*16*78\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(in_channels=16, out_channels=32, kernel_size=4, stride=2, bias=False),#n*16*78->n*32*38\n",
    "    nn.BatchNorm1d(num_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, stride=2, bias=False),#n*32*38->n*64*18\n",
    "    nn.BatchNorm1d(num_features=64),\n",
    "    nn.ReLU(),\n",
    "    Flatten(out_features=64*18),\n",
    "    nn.Linear(in_features=64*18, out_features=256),\n",
    "    nn.ReLU(),\n",
    "    VariationalLayer(in_features=256, out_features=LATENT_DIM, return_KL=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = Encoder_with_y(ORIGINAL_DIM, LATENT_DIM, Y_DIM)\n",
    "#encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    nn.Linear(in_features=LATENT_DIM+Y_DIM, out_features=256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=256, out_features=64*18),\n",
    "    Reshape(out_shape=(64,18)),\n",
    "    nn.BatchNorm1d(num_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=0),\n",
    "    nn.BatchNorm1d(num_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=0),\n",
    "    #nn.BatchNorm1d(num_features=16),\n",
    "    nn.ReLU(),\n",
    "    ConvTransposeDecoderOutput(\n",
    "        in_channels=16, \n",
    "        in_features=16*78, \n",
    "        out_features=ORIGINAL_DIM, \n",
    "        kernel_size=6, \n",
    "        stride=2\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2 = nn.Sequential(\n",
    "    nn.Linear(in_features=LATENT_DIM+Y_DIM, out_features=256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=256, out_features=64*18),\n",
    "    Reshape(out_shape=(64,18)),\n",
    "    nn.BatchNorm1d(num_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=0),\n",
    "    nn.BatchNorm1d(num_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=0),\n",
    "    #nn.BatchNorm1d(num_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose1d(in_channels=16, out_channels=1, kernel_size=6, stride=2, padding=0),\n",
    "    Flatten(out_features=160)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nn.Sequential(\n",
    "            Reshape(out_shape=(1,160)),\n",
    "            InceptionBlock(\n",
    "                in_channels=1, \n",
    "                n_filters=32, \n",
    "                kernel_sizes=[5, 11, 23],\n",
    "                bottleneck_channels=32,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=32*4, \n",
    "                n_filters=32, \n",
    "                kernel_sizes=[5, 11, 23],\n",
    "                bottleneck_channels=32,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            Flatten(out_features=32*4*1),\n",
    "            nn.Linear(in_features=4*32*1, out_features=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = nn.Sequential(\n",
    "            Reshape(out_shape=(1,160)),\n",
    "            InceptionBlock(\n",
    "                in_channels=1, \n",
    "                n_filters=32, \n",
    "                kernel_sizes=[5, 11, 23],\n",
    "                bottleneck_channels=32,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            Flatten(out_features=32*4*160),\n",
    "            nn.Linear(in_features=4*32*160, out_features=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = nn.Sequential(\n",
    "    Reshape(out_shape=(1, ORIGINAL_DIM)),\n",
    "    nn.Conv1d(in_channels=1, out_channels=16, kernel_size=6, stride=2, padding=0, bias=False), # n*1*160 -> n*16*78\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(in_channels=16, out_channels=32, kernel_size=4, stride=2, bias=False),#n*16*78->n*32*38\n",
    "    nn.BatchNorm1d(num_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, stride=2, bias=False),#n*32*38->n*64*18\n",
    "    nn.BatchNorm1d(num_features=64),\n",
    "    nn.ReLU(),\n",
    "    Flatten(out_features=64*18),\n",
    "    nn.Linear(in_features=64*18, out_features=256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=256, out_features=Y_DIM)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(MSE, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        if self.reduction==\"mean\":\n",
    "            return - (torch.mean(torch.sum((y_true-y_pred).pow(2), axis=1)))\n",
    "        if self.reduction==\"none\":\n",
    "            return - (torch.sum((y_true-y_pred).pow(2), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemiSupervisedGenerativeModel(\n",
    "                            encoder=nn.Sequential(\n",
    "                                nn.Linear(in_features=160, out_features=400),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(in_features=400, out_features=200),\n",
    "                                nn.ELU(),\n",
    "                                VariationalLayer(in_features=200, out_features=30, return_KL=False)), \n",
    "                            decoder=nn.Sequential(\n",
    "                                nn.Linear(in_features=30+4, out_features=200),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(in_features=200, out_features=400),\n",
    "                                nn.ELU(),\n",
    "                                VariationalDecoderOutput(in_features=400, out_features=160)\n",
    "                                ),\n",
    "                            classifier=nn.Sequential(\n",
    "                                nn.Linear(in_features=160, out_features=400),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(in_features=400, out_features=200),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(in_features=200, out_features=4)\n",
    "                                ),\n",
    "                            y_dim=Y_DIM,\n",
    "                            include_y=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "Generative_Model_Trainer(\n",
      "  (model): SemiSupervisedGenerativeModel(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=160, out_features=400, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=400, out_features=200, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): VariationalLayer(\n",
      "        (mu): Linear(in_features=200, out_features=30, bias=True)\n",
      "        (rho): Linear(in_features=200, out_features=30, bias=True)\n",
      "        (softplus): Softplus(beta=1, threshold=20)\n",
      "      )\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=34, out_features=200, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=200, out_features=400, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): VariationalDecoderOutput(\n",
      "        (mu): Linear(in_features=400, out_features=160, bias=True)\n",
      "        (rho): Linear(in_features=400, out_features=1, bias=True)\n",
      "        (softplus): Softplus(beta=1, threshold=20)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=160, out_features=400, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=400, out_features=200, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=200, out_features=4, bias=True)\n",
      "    )\n",
      "    (likelihood): Gaussian_NLL()\n",
      "    (one_hot): One_Hot()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gmt = Generative_Model_Trainer(\n",
    "    model=model, \n",
    "    optimizer=torch.optim.Adam, \n",
    "    scheduler=None, \n",
    "    lr=1e-3, \n",
    "    tensorboard=True, \n",
    "    model_name=\"InceptionTime_mod+VAE_lr-1e-3_bs-512\", \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], average_loss:-131.2817, validation_loss:-230.6921, val_accuracy:0.6245\n",
      "Epoch [2/20], average_loss:-275.2300, validation_loss:-285.7618, val_accuracy:0.6243\n",
      "Epoch [3/20], average_loss:-326.9514, validation_loss:-305.2858, val_accuracy:0.6244\n",
      "Epoch [4/20], average_loss:-337.1337, validation_loss:-316.5893, val_accuracy:0.6244\n",
      "Epoch [5/20], average_loss:-350.3854, validation_loss:-334.0041, val_accuracy:0.6244\n",
      "Epoch [6/20], average_loss:-356.4182, validation_loss:-340.2420, val_accuracy:0.6244\n",
      "Epoch [7/20], average_loss:-345.3538, validation_loss:-353.6735, val_accuracy:0.6244\n",
      "Epoch [8/20], average_loss:-378.8522, validation_loss:-360.2030, val_accuracy:0.6244\n",
      "Epoch [9/20], average_loss:nan, validation_loss:nan, val_accuracy:0.2530\n",
      "Epoch [10/20], average_loss:nan, validation_loss:nan, val_accuracy:0.2530\n",
      "Epoch [11/20], average_loss:nan, validation_loss:nan, val_accuracy:0.2530\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-c2e57ec76f21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupervised_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsupervised_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_unsup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Semi-supervised-Learning-with-VAE\\utils\\generative_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, epochs, supervised_dataset, unsupervised_dataset, validation_dataset, batch_size)\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m             \u001b[1;31m# ============= validation =============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gmt(epochs=range(20), supervised_dataset=sup, unsupervised_dataset = X_unsup, validation_dataset=test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3863, 1.3863, 1.3863])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66944, 160])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
